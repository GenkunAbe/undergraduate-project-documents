\chapter{项目内容及算法实现}\label{sec:algorithm}

本章介绍了项目在准备阶段和项目主体部分各算法的实现细节。项目的准备阶段包括原始视频处理、行人检测以及人工标注行人标签。项目的主体部分包括行人重识别算法的复现、多CPU集群分布式训练的实现以及强化学习算法的实现。

\section{数据采集和预处理}

\subsection{数据采集}

经过路线规划、场地布置、演员召集等工作，目前完成了数据库的预拍摄工作。数据集的正式拍摄计划于 2018 年 5 月 18 日和 19 日进行，计划召集 500 余名演员参与。第一次预拍摄过程的详细情况如下：

第一次预拍摄的参与者为 19 人，其中有 15 人完整地走完了路线，全程拍摄时长大约为 25 分钟。视频拍摄的地点位于中山大学数据科学与计算机学院楼外停车场与楼内 1、2、3、4、6 楼的大厅和走廊。视频拍摄过程中需要在若干个关键点（某个摄像头或者某一楼层）设置人员负责时间点记录和人流控制，记下每一个演员进入视频画面的时间点，用于之后的视频分割。

第一次预拍摄一共使用了 21 个摄像头，最终 20 个摄像头有视频输出。其中原有的摄像头个数为 14 个，新增 DV 5 台，其中有 1 台不能正常写入视频文件，新增手机 2 部。原有的监控视频的分辨率大部分为 1280$\times$720（720P），也有部分是 1920$\times$1080（1080P），新增的 DV 拍摄的视频分辨率都是 1080P，新增的 2 部手机的分辨率分别为 720P 和 1080P。视频的帧率均为 25 FPS。

最终用于本项目的数据集包含 17 个摄像头的视频数据，每个摄像头的视频数据包含在一个视频文件内，视频的长度约为 10 分钟，出现的演员有 15 人。

\subsection{原始视频处理}

\subsubsection{视频转码}
原有的摄像头输出的视频文件的时间轴有误，原本时长为 20 分钟的视频却显示有 13 个小时。
部分原始视频文件偶尔会出现一两帧丢失的情况，不是准确的 25 FPS。
原始视频被分割成了 2 $\sim$ 3 个视频文件。
所以需要用 FFmpeg 进行转码。转码保持原有的分辨率不变，填补少量的丢失帧，最终输出正常的视频文件。
此阶段的输出是：每个摄像头对应一个视频文件。

\subsubsection{时间点校正和估计}
记录员的对于每一个演员的记录内容为：演员进入视频画面的时间点和演员的标签。将记录员记录的第一个时间点与视频中第一个演员出现的时间点对齐，即可得到每个演员进入视频画面的时间点。
对于那些没有记录员的摄像头，则采用“进入上一个摄像头的时间 + 两摄像头之间平均行走时间”的方法来估计。
由于记录的时候没有精确按照点位进行记录，所以有部分的记录数据需要对照视频重新调整时间点。
此阶段的输出是：每一个演员的进入每一个摄像头的时间点 + 该演员的标签。

\subsubsection{视频切割}
有了每一个演员演员进入每一个摄像头的时间点，然后再分别估计每一个摄像头画面中演员停留的平均时间，即可得到每一个演员在每一个视频文件中的时间段，从而利用 FFmpeg 将每一个演员在没一个摄像头的视频片断切割出来。
此阶段的输出是：每个演员在每个摄像头的视频片断。

\subsection{行人检测}
行人识别属于计算机视觉领域中的目标识别（Object Detection）问题，目前目标识别问题state-of-the-art 是何凯明等人的Mask RCNN\cite{he2017mask}模型。Mask RCNN在Faster RCNN\cite{ren2015faster}的基础上，添加了一条网络分支，以实现在目标检测的同时，将每个像素分割出来，得到高质量的分割结果。

在本项目中，使用了Mask RCNN作者公开的源代码项目Detectron\cite{Detectron2018}作为行人检测工具，该项目中包含预训练的网络参数和目标检测API，预训练模型在训练过程中使用了COCO 2014\cite{lin2014microsoft}数据集，模型的后端深度神经网络使用了ResNet-101\cite{he2016deep}网络。

对于每个视频文件，遍历其所有的画面帧，调用Detectron API检测画面当中的行人，再根据记录员的记录得到画面中行人的标签。由于画面中可能存在多于一个行人，所以此步骤会将不属于演员的路人也打上演员的标签，需要后期的修正。对于画面中的每一个行人，输出一条记录，其包含的字段分别为：摄像头ID、演员ID、当前帧数、当前帧内行人个数、Bounding Box的左上角与右下角坐标、行人的概率。

\subsection{人工标记行人的标签}

由于 Detectron 无法区分在同一帧内的演员和路人，暂时输出了相同的标签，所以需要进一步地用人工把那些路人的标签标记为 -1。

人工标记的过程借助了事先实现的行人重识别baseline，具体过程如下：

将记录中当前帧内行人个数为1的记录挑选出来并按照标签分组，由于该帧中行人数量为 1，所以基本可以确定标签的正确性。把每个 Bounding Box 输入行人重识别 baseline，得到该人的特征。再求标签相同的特征的均值，得到可以代表每一个演员的特征向量。

对于记录中当前帧内行人个数大于1的部分，按照摄像头ID、标签、当前帧数分组，每一组内就是同一摄像头下同一帧内的所有行人的记录。计算每个行人的特征向量与该标签演员的特征向量之间的距离，将距离最近的那条记录的标签标记不变，其它记录的标签改为-1。此时每一个当前帧内行人个数大于1的组内只有一条记录的标签不为-1。

将每个组内标签不为 -1 的图像输出文件，摄像头ID及标签相同的图片放入同一个文件夹，人工评估每个文件夹内的标注结果。删除标注出错的图片，剩余的图片最能代表该场景、该演员的特征，所以求该文件夹内剩余图片的特征向量的均值，作为代表该演员在该场景下的特征向量。

扫描每个文件夹内缺失的图片（文件缺失代表在上一步被删除了，自动标注出错，需要进一步标注），按照与之前步骤类似的做法，找出与对应文件夹内特征向量最近的记录，将其它记录的标签标记为 -1，再次输出图片到文件，人工检查结果。

将每个文件夹内标注出错的结果人工删除。用代码扫描被删除的图片，将原始记录中该帧图像中所有的记录删除。因为到此步骤标注出错的情况很少，所以仅删除了少量的记录。

\section{行人重识别领域state-of-the-art论文复现}

\subsection{读入训练/测试数据}
Market1501数据集的格式是原始的JPG图片，所以需要加载自己的训练集，这种情况下最好还是继承 Dataset 类比较方便。

Dataset 类的本质是定义了数据所在的位置，以及数据需要预处理的方法。至于数据的位置是在硬盘里面还是提前加载到内存里面，由该类的内部实现决定。由于Market1501数据集过大，所以本项目采用了分批次读取的做法，在训练和测试过程中使用20个进程将数据从硬盘加载到内存中。

\subsection{模型训练}

在本项目中，模型的训练使用了Market1501\cite{zheng2015scalable}数据集，数据集使用了水平平移翻转和正则化作为数据扩充的方法。训练模型分为三个阶段，第一阶段是标准的PCB训练，即均匀地分割Feature Map的各个部分。此阶段一共包含60个Epoch，Batch Size为64，模型新增的卷积层的参数初始化为服从正态分布$N(0, 0.001^2)$的随机数。ResNet50网络参数的学习率为0.01，其它参数的学习率为0.1，当Epoch大于或等于40时，各学习率分别缩小为初始的$1/10$。训练模型的第二阶段将PCB中均匀分割改为分类，并且固定第一阶段训练的所有参数，只学习分类层的参数，训练的Epoch数和Batch Size与第一阶段相同。分类层的参数同样初始化为服从正态分布$N(0, 0.001^2)$的随机数，初始学习率为0.01，当Epoch大于或等于40时，学习率缩小为初始的$1/10$。训练模型的第三阶段为调整整个网络中所有的参数，训练的Epoch数和Batch Size与第一阶段相同。所有参数的初始学习率为0.01，当Epoch大于或等于40时，学习率缩小为初始的$1/10$。

\subsection{特征提取}

在特征提取阶段，将最后用于分类任务的全连接层移除，并将全连接层移除后网络最后一层或倒数第二层输出的$p$个向量拼接起来，得到行人的特征表示。在测试阶段，采用欧氏距离衡量各特征之间的相似度。

\section{强化学习框架实现}

强化学习框架实现了Q-Learning算法以及监控摄像头部署方案评价指标计算函数。在Q-Learning算法中，智能体的生命周期数$N$为100000，状态的长期价值$Q$的学习率$\alpha$为0.01，智能体的远见性$\gamma$为0.2。

\section{面向CPU集群的分布式深度学习训练框架}

若要使用分布式训练，需要初始化各节点与master节点的通信连接，然后等待master节点发来的训练任务。